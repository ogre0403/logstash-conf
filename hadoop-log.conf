input {
    file{
        path => ["/opt/hadoop/logs/hadoop-hdadm-namenode-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/namenode.since"
        add_field =>{
            "nodetype" => "namenode"
        }
    }

    
    file{
        path => ["/opt/hadoop/logs/yarn-hdadm-resourcemanager-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/resourcemanager.since"
        add_field => {
            "nodetype" => "resourcemanager"
        } 
    }
    
    file{
        path => ["/opt/hadoop/logs/hadoop-hdadm-secondarynamenode-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/ssn.since"
        add_field => {
            "nodetype" => "secondaryNN"
        } 
    }

    file{
        path => ["/opt/hadoop/logs/hadoop-hdadm-datanode-TCDN*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/datanode.since"
        add_field => {
            "nodetype" => "datanode"
        } 
    }

    file{
        path => ["/opt/hadoop/logs/yarn-hdadm-nodemanager-TCDN*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/nodemanager.since"
        add_field => {
            "nodetype" => "nodemanager"
        } 
    }

    file{
        path => ["/opt/hadoop/logs/hadoop-hdadm-journalnode-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/journal.since"
        add_field => {
            "nodetype" => "journalnode"
        } 
    }

    file{
        path => ["/opt/hadoop/logs/hadoop-hdadm-zkfc-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/zkfc.since"
        add_field => {
            "nodetype" => "zkfc"
        } 
    }

    file{
        path => ["/opt/hbase/logs/hbase-hdadm-master-TC*.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/master.since"
        add_field => {
            "nodetype" => "master"
        } 
    }

    file{
        path => ["/opt/storm/logs/nimbus.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/nimbus.since"
        add_field => {
            "nodetype" => "nimbus"
        } 
    }

    file{
        path => ["/opt/storm/logs/supervisor.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/supervisor.since"
        add_field => {
            "nodetype" => "supervisor"
        } 
    }

    file{
        path => ["/opt/storm/logs/ui.log"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/ui.since"
        add_field => {
            "nodetype" => "ui"
        } 
    }

    file{
        path => ["/data0/yarn/logs/application_*/container_*/syslog",
                 "/data1/yarn/logs/application_*/container_*/syslog",
                 "/data2/yarn/logs/application_*/container_*/syslog",
                 "/data3/yarn/logs/application_*/container_*/syslog"]
        #start_position => "beginning"
        sincedb_path => "/opt/logstash/.sincedb/mrjob.since"
        add_field => {
            "nodetype" => "mrjob"
        }
    }
}

filter {
    # hadoop log4j log start with ISO8601 date format, following
    # lines not start with date should be merged into single event
    multiline{
        enable_flush => true
        pattern => "^%{TIMESTAMP_ISO8601}"
        what => "previous"
        negate => true
    }
    
    # grok CANNOT hadnle string with \n, replace \n first
    mutate {
        gsub => ['message', "\n", " @LINE_BREAK@ "]
    }
    
    # match hadoop log4j format 
    grok {
        match => [message, "%{DATA:ts} %{WORD:level} %{JAVACLASS:class}: %{GREEDYDATA:log_msg}" ]
    }
    
    # use log date as timestamp
    date {
        match => [ "ts", "yyyy-MM-dd HH:mm:ss,SSS"]
        timezone =>  "Asia/Taipei"
    }
    
    mutate {
        gsub => ['log_msg', "@LINE_BREAK@", "\n"]
        # The new line here is the only why to put a new line ...
        gsub => ["log_msg", "\\n", "\
"]
        gsub => ["log_msg", "\\$", "" ]
    }

    if "_grokparsefailure" in [tags] {
         # grok not match, leave message field for debug
         mutate {                            
            remove_field => [ "tags" ]      
        }
    } else {
         # grok match, remove extra field
         mutate {                                        
            remove_field => [ "tags", "message", "ts" ]
        }                                               
    }

    if [nodetype] == "mrjob" {
        mutate {
            add_field => { "tmppath" => "%{path}"}
        }
        csv {
            source => "tmppath"
            separator => "/"
        }
        mutate {
            add_field => {
                "jobID" => "%{column5}"
            }
            remove_field => ["tmppath","column1", "column2", "column3","column4","column5","column6","column7"]
        }
    } else {
        mutate{
            remove_field => [ "jobID" ]
        }
    }
}

output { 
    #stdout{
    #    codec => rubydebug
    #}
    elasticsearch {
        host => TCJN
    }

#    kafka { 
#        broker_list => "slave1:2092,master:2092"
#        topic_id => "logstash"
#        client_id => "log_shipper_slave1"
#    } 
}
